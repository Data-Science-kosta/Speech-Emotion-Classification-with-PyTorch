{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sbRkAw1RVNbn"
   },
   "source": [
    "# Load file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "id": "20SnQEifVNbq"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import IPython\n",
    "from IPython.display import Audio\n",
    "from IPython.display import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "EMOTIONS = {1:'neutral', 2:'calm', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 0:'surprise'} # surprise je promenjen sa 8 na 0\n",
    "DATA_PATH = '../input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/'\n",
    "SAMPLE_RATE = 48000\n",
    "\n",
    "data = pd.DataFrame(columns=['Emotion', 'Emotion intensity', 'Gender','Path'])\n",
    "for dirname, _, filenames in os.walk(DATA_PATH):\n",
    "    for filename in filenames:\n",
    "        file_path = os.path.join('/kaggle/input/',dirname, filename)\n",
    "        identifiers = filename.split('.')[0].split('-')\n",
    "        emotion = (int(identifiers[2]))\n",
    "        if emotion == 8: # promeni surprise sa 8 na 0\n",
    "            emotion = 0\n",
    "        if int(identifiers[3]) == 1:\n",
    "            emotion_intensity = 'normal' \n",
    "        else:\n",
    "            emotion_intensity = 'strong'\n",
    "        if int(identifiers[6])%2 == 0:\n",
    "            gender = 'female'\n",
    "        else:\n",
    "            gender = 'male'\n",
    "        \n",
    "        data = data.append({\"Emotion\": emotion,\n",
    "                            \"Emotion intensity\": emotion_intensity,\n",
    "                            \"Gender\": gender,\n",
    "                            \"Path\": file_path\n",
    "                             },\n",
    "                             ignore_index = True\n",
    "                          )\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "id": "Sf5D86-hVNbs"
   },
   "outputs": [],
   "source": [
    "print(\"number of files is {}\".format(len(data)))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YwW1ZFlcVNbs"
   },
   "source": [
    "number of examples per Emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ERBf93UVNbt"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.bar(x=range(8), height=data['Emotion'].value_counts())\n",
    "ax.set_xticks(ticks=range(8))\n",
    "ax.set_xticklabels([EMOTIONS[i] for i in range(8)],fontsize=10)\n",
    "ax.set_xlabel('Emotion')\n",
    "ax.set_ylabel('Number of examples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-vKaQhFmVNbu"
   },
   "source": [
    "number of examples per gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fJFLgtVeVNbu"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "counts = data['Gender'].value_counts()\n",
    "ax.bar(x=[0,1], height=counts.values)\n",
    "ax.set_xticks(ticks=[0,1])\n",
    "ax.set_xticklabels(list(counts.index))\n",
    "ax.set_xlabel('Gender')\n",
    "ax.set_ylabel('Number of examples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jK-7T0x-VNbv"
   },
   "source": [
    "number of examples per emotion intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T7MdEBwgVNbv"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "counts = data['Emotion intensity'].value_counts()\n",
    "ax.bar(x=[0,1], height=counts.values)\n",
    "ax.set_xticks(ticks=[0,1])\n",
    "ax.set_xticklabels(list(counts.index))\n",
    "ax.set_xlabel('Gender')\n",
    "ax.set_ylabel('Number of examples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GmTzEdCkVNbw"
   },
   "source": [
    "# Load the signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6K6k60yuVNbw"
   },
   "outputs": [],
   "source": [
    "mel_spectrograms = []\n",
    "signals = []\n",
    "for i, file_path in enumerate(data.Path):\n",
    "    audio, sample_rate = librosa.load(file_path, duration=3, offset=0.5, sr=SAMPLE_RATE)\n",
    "    signal = np.zeros((int(SAMPLE_RATE*3,)))\n",
    "    signal[:len(audio)] = audio\n",
    "    signals.append(signal)\n",
    "    print(\"\\r Processed {}/{} files\".format(i,len(data)),end='')\n",
    "signals = np.stack(signals,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PQqGSjQLVNbx"
   },
   "source": [
    "# Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D3uzoW2cVNbx"
   },
   "outputs": [],
   "source": [
    "X = signals\n",
    "train_ind,test_ind,val_ind = [],[],[]\n",
    "X_train,X_val,X_test = [],[],[]\n",
    "Y_train,Y_val,Y_test = [],[],[]\n",
    "for emotion in range(len(EMOTIONS)):\n",
    "    emotion_ind = list(data.loc[data.Emotion==emotion,'Emotion'].index)\n",
    "    emotion_ind = np.random.permutation(emotion_ind)\n",
    "    m = len(emotion_ind)\n",
    "    ind_train = emotion_ind[:int(0.8*m)]\n",
    "    ind_val = emotion_ind[int(0.8*m):int(0.9*m)]\n",
    "    ind_test = emotion_ind[int(0.9*m):]\n",
    "    X_train.append(X[ind_train,:])\n",
    "    Y_train.append(np.array([emotion]*len(ind_train),dtype=np.int32))\n",
    "    X_val.append(X[ind_val,:])\n",
    "    Y_val.append(np.array([emotion]*len(ind_val),dtype=np.int32))\n",
    "    X_test.append(X[ind_test,:])\n",
    "    Y_test.append(np.array([emotion]*len(ind_test),dtype=np.int32))\n",
    "    train_ind.append(ind_train)\n",
    "    test_ind.append(ind_test)\n",
    "    val_ind.append(ind_val)\n",
    "X_train = np.concatenate(X_train,0)\n",
    "X_val = np.concatenate(X_val,0)\n",
    "X_test = np.concatenate(X_test,0)\n",
    "Y_train = np.concatenate(Y_train,0)\n",
    "Y_val = np.concatenate(Y_val,0)\n",
    "Y_test = np.concatenate(Y_test,0)\n",
    "train_ind = np.concatenate(train_ind,0)\n",
    "val_ind = np.concatenate(val_ind,0)\n",
    "test_ind = np.concatenate(test_ind,0)\n",
    "print(f'X_train:{X_train.shape}, Y_train:{Y_train.shape}')\n",
    "print(f'X_val:{X_val.shape}, Y_val:{Y_val.shape}')\n",
    "print(f'X_test:{X_test.shape}, Y_test:{Y_test.shape}')\n",
    "# check if all are unique\n",
    "unique, count = np.unique(np.concatenate([train_ind,test_ind,val_ind],0), return_counts=True)\n",
    "print(\"Number of unique indexes is {}, out of {}\".format(sum(count==1), X.shape[0]))\n",
    "\n",
    "del X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WoAJ6nZzVNbx"
   },
   "source": [
    "# Augment signals by adding AWGN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pZBJy6P5VNby"
   },
   "outputs": [],
   "source": [
    "def addAWGN(signal, num_bits=16, augmented_num=2, snr_low=15, snr_high=30): \n",
    "    signal_len = len(signal)\n",
    "    # Generate White Gaussian noise\n",
    "    noise = np.random.normal(size=(augmented_num, signal_len))\n",
    "    # Normalize signal and noise\n",
    "    norm_constant = 2.0**(num_bits-1)\n",
    "    signal_norm = signal / norm_constant\n",
    "    noise_norm = noise / norm_constant\n",
    "    # Compute signal and noise power\n",
    "    s_power = np.sum(signal_norm ** 2) / signal_len\n",
    "    n_power = np.sum(noise_norm ** 2, axis=1) / signal_len\n",
    "    # Random SNR: Uniform [15, 30] in dB\n",
    "    target_snr = np.random.randint(snr_low, snr_high)\n",
    "    # Compute K (covariance matrix) for each noise \n",
    "    K = np.sqrt((s_power / n_power) * 10 ** (- target_snr / 10))\n",
    "    K = np.ones((signal_len, augmented_num)) * K  \n",
    "    # Generate noisy signal\n",
    "    return signal + K.T * noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CgwTVlxJVNby"
   },
   "outputs": [],
   "source": [
    "aug_signals = []\n",
    "aug_labels = []\n",
    "for i in range(X_train.shape[0]):\n",
    "    signal = X_train[i,:]\n",
    "    augmented_signals = addAWGN(signal)\n",
    "    for j in range(augmented_signals.shape[0]):\n",
    "        aug_labels.append(data.loc[i,\"Emotion\"])\n",
    "        aug_signals.append(augmented_signals[j,:])\n",
    "        data = data.append(data.iloc[i], ignore_index=True)\n",
    "    print(\"\\r Processed {}/{} files\".format(i,X_train.shape[0]),end='')\n",
    "aug_signals = np.stack(aug_signals,axis=0)\n",
    "X_train = np.concatenate([X_train,aug_signals],axis=0)\n",
    "aug_labels = np.stack(aug_labels,axis=0)\n",
    "Y_train = np.concatenate([Y_train,aug_labels])\n",
    "print('')\n",
    "print(f'X_train:{X_train.shape}, Y_train:{Y_train.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tdrZXT5hVNbz"
   },
   "source": [
    "# Calculate mel spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eiWEtIfuVNb0"
   },
   "outputs": [],
   "source": [
    "def getMELspectrogram(audio, sample_rate):\n",
    "    mel_spec = librosa.feature.melspectrogram(y=audio,\n",
    "                                              sr=sample_rate,\n",
    "                                              n_fft=1024,\n",
    "                                              win_length = 512,\n",
    "                                              window='hamming',\n",
    "                                              hop_length = 256,\n",
    "                                              n_mels=128,\n",
    "                                              fmax=sample_rate/2\n",
    "                                             )\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    return mel_spec_db\n",
    "\n",
    "# test function\n",
    "audio, sample_rate = librosa.load(data.loc[0,'Path'], duration=3, offset=0.5,sr=SAMPLE_RATE)\n",
    "signal = np.zeros((int(SAMPLE_RATE*3,)))\n",
    "signal[:len(audio)] = audio\n",
    "mel_spectrogram = getMELspectrogram(signal, SAMPLE_RATE)\n",
    "librosa.display.specshow(mel_spectrogram, y_axis='mel', x_axis='time')\n",
    "print('MEL spectrogram shape: ',mel_spectrogram.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uekXrF7BVNb0"
   },
   "outputs": [],
   "source": [
    "mel_train = []\n",
    "print(\"Calculatin mel spectrograms for train set\")\n",
    "for i in range(X_train.shape[0]):\n",
    "    mel_spectrogram = getMELspectrogram(X_train[i,:], sample_rate=SAMPLE_RATE)\n",
    "    mel_train.append(mel_spectrogram)\n",
    "    print(\"\\r Processed {}/{} files\".format(i,X_train.shape[0]),end='')\n",
    "print('')\n",
    "del X_train\n",
    "\n",
    "mel_val = []\n",
    "print(\"Calculatin mel spectrograms for val set\")\n",
    "for i in range(X_val.shape[0]):\n",
    "    mel_spectrogram = getMELspectrogram(X_val[i,:], sample_rate=SAMPLE_RATE)\n",
    "    mel_val.append(mel_spectrogram)\n",
    "    print(\"\\r Processed {}/{} files\".format(i,X_val.shape[0]),end='')\n",
    "print('')\n",
    "del X_val\n",
    "\n",
    "mel_test = []\n",
    "print(\"Calculatin mel spectrograms for test set\")\n",
    "for i in range(X_test.shape[0]):\n",
    "    mel_spectrogram = getMELspectrogram(X_test[i,:], sample_rate=SAMPLE_RATE)\n",
    "    mel_test.append(mel_spectrogram)\n",
    "    print(\"\\r Processed {}/{} files\".format(i,X_test.shape[0]),end='')\n",
    "print('')\n",
    "del X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sNXWko_hVNb1"
   },
   "source": [
    "# Split into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iKl22PFbVNb1"
   },
   "outputs": [],
   "source": [
    "def splitIntoChunks(mel_spec,win_size,stride):\n",
    "    t = mel_spec.shape[1]\n",
    "    num_of_chunks = int(t/stride)\n",
    "    chunks = []\n",
    "    for i in range(num_of_chunks):\n",
    "        chunk = mel_spec[:,i*stride:i*stride+win_size]\n",
    "        if chunk.shape[1] == win_size:\n",
    "            chunks.append(chunk)\n",
    "    return np.stack(chunks,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OADpVxkBVNb1"
   },
   "outputs": [],
   "source": [
    "# get chunks\n",
    "# train set\n",
    "mel_train_chunked = []\n",
    "for mel_spec in mel_train:\n",
    "    chunks = splitIntoChunks(mel_spec, win_size=128,stride=64)\n",
    "    mel_train_chunked.append(chunks)\n",
    "print(\"Number of chunks is {}\".format(chunks.shape[0]))\n",
    "# val set\n",
    "mel_val_chunked = []\n",
    "for mel_spec in mel_val:\n",
    "    chunks = splitIntoChunks(mel_spec, win_size=128,stride=64)\n",
    "    mel_val_chunked.append(chunks)\n",
    "print(\"Number of chunks is {}\".format(chunks.shape[0]))\n",
    "# test set\n",
    "mel_test_chunked = []\n",
    "for mel_spec in mel_test:\n",
    "    chunks = splitIntoChunks(mel_spec, win_size=128,stride=64)\n",
    "    mel_test_chunked.append(chunks)\n",
    "print(\"Number of chunks is {}\".format(chunks.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kzQy-0JBVNb2"
   },
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vapt6LfNVNb3"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# BATCH FIRST TimeDistributed layer\n",
    "class TimeDistributed(nn.Module):\n",
    "    def __init__(self, module):\n",
    "        super(TimeDistributed, self).__init__()\n",
    "        self.module = module\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if len(x.size()) <= 2:\n",
    "            return self.module(x)\n",
    "        # squash samples and timesteps into a single axis\n",
    "        elif len(x.size()) == 3: # (samples, timesteps, inp1)\n",
    "            x_reshape = x.contiguous().view(-1, x.size(2))  # (samples * timesteps, inp1)\n",
    "        elif len(x.size()) == 4: # (samples,timesteps,inp1,inp2)\n",
    "            x_reshape = x.contiguous().view(-1, x.size(2), x.size(3)) # (samples*timesteps,inp1,inp2)\n",
    "        else: # (samples,timesteps,inp1,inp2,inp3)\n",
    "            x_reshape = x.contiguous().view(-1, x.size(2), x.size(3),x.size(4)) # (samples*timesteps,inp1,inp2,inp3)\n",
    "            \n",
    "        y = self.module(x_reshape)\n",
    "        \n",
    "        # we have to reshape Y\n",
    "        if len(x.size()) == 3:\n",
    "            y = y.contiguous().view(x.size(0), -1, y.size(1))  # (samples, timesteps, out1)\n",
    "        elif len(x.size()) == 4:\n",
    "            y = y.contiguous().view(x.size(0), -1, y.size(1), y.size(2)) # (samples, timesteps, out1,out2)\n",
    "        else:\n",
    "            y = y.contiguous().view(x.size(0), -1, y.size(1), y.size(2),y.size(3)) # (samples, timesteps, out1,out2, out3)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HLG4M32EVNb3"
   },
   "outputs": [],
   "source": [
    "class HybridModel(nn.Module):\n",
    "    def __init__(self,num_emotions):\n",
    "        super().__init__()\n",
    "        # conv block\n",
    "        self.conv2Dblock = nn.Sequential(\n",
    "            # 1. conv block\n",
    "            TimeDistributed(nn.Conv2d(in_channels=1,\n",
    "                                   out_channels=16,\n",
    "                                   kernel_size=3,\n",
    "                                   stride=1,\n",
    "                                   padding=1\n",
    "                                  )),\n",
    "            TimeDistributed(nn.BatchNorm2d(16)),\n",
    "            TimeDistributed(nn.ReLU()),\n",
    "            TimeDistributed(nn.MaxPool2d(kernel_size=2, stride=2)),\n",
    "            TimeDistributed(nn.Dropout(p=0.4)),\n",
    "            # 2. conv block\n",
    "            TimeDistributed(nn.Conv2d(in_channels=16,\n",
    "                                   out_channels=32,\n",
    "                                   kernel_size=3,\n",
    "                                   stride=1,\n",
    "                                   padding=1\n",
    "                                  )),\n",
    "            TimeDistributed(nn.BatchNorm2d(32)),\n",
    "            TimeDistributed(nn.ReLU()),\n",
    "            TimeDistributed(nn.MaxPool2d(kernel_size=4, stride=4)),\n",
    "            TimeDistributed(nn.Dropout(p=0.4)),\n",
    "            # 3. conv block\n",
    "            TimeDistributed(nn.Conv2d(in_channels=32,\n",
    "                                   out_channels=64,\n",
    "                                   kernel_size=3,\n",
    "                                   stride=1,\n",
    "                                   padding=1\n",
    "                                  )),\n",
    "            TimeDistributed(nn.BatchNorm2d(64)),\n",
    "            TimeDistributed(nn.ReLU()),\n",
    "            TimeDistributed(nn.MaxPool2d(kernel_size=4, stride=4)),\n",
    "            TimeDistributed(nn.Dropout(p=0.4)),\n",
    "            # 4. conv block\n",
    "            TimeDistributed(nn.Conv2d(in_channels=64,\n",
    "                                   out_channels=128,\n",
    "                                   kernel_size=3,\n",
    "                                   stride=1,\n",
    "                                   padding=1\n",
    "                                  )),\n",
    "            TimeDistributed(nn.BatchNorm2d(128)),\n",
    "            TimeDistributed(nn.ReLU()),\n",
    "            TimeDistributed(nn.MaxPool2d(kernel_size=4, stride=4)),\n",
    "            TimeDistributed(nn.Dropout(p=0.4))\n",
    "        )\n",
    "        # LSTM block\n",
    "        hidden_size = 64\n",
    "        self.lstm = nn.LSTM(input_size=128,hidden_size=hidden_size,bidirectional=False, batch_first=True) \n",
    "        self.dropout_lstm = nn.Dropout(p=0.3)\n",
    "        # Linear softmax layer\n",
    "        self.out_linear = nn.Linear(hidden_size,num_emotions)\n",
    "    def forward(self,x):\n",
    "        conv_embedding = self.conv2Dblock(x)\n",
    "        conv_embedding = torch.flatten(conv_embedding, start_dim=2) # do not flatten batch dimension and time\n",
    "        lstm_embedding, (h,c) = self.lstm(conv_embedding)\n",
    "        lstm_embedding = self.dropout_lstm(lstm_embedding)\n",
    "        # lstm_embedding (batch, time, hidden_size)\n",
    "        lstm_output = lstm_embedding[:,-1,:] \n",
    "        output_logits = self.out_linear(lstm_output)\n",
    "        output_softmax = nn.functional.softmax(output_logits,dim=1)\n",
    "        return output_logits, output_softmax\n",
    "                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ez_7eMZoVNb4"
   },
   "outputs": [],
   "source": [
    "def loss_fnc(predictions, targets):\n",
    "    return nn.CrossEntropyLoss()(input=predictions,target=targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uo1iRY8yVNb5"
   },
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DsDMnMs3VNb5"
   },
   "outputs": [],
   "source": [
    "def make_train_step(model, loss_fnc, optimizer):\n",
    "    def train_step(X,Y):\n",
    "        # set model to train mode\n",
    "        model.train()\n",
    "        # forward pass\n",
    "        output_logits, output_softmax = model(X)\n",
    "        predictions = torch.argmax(output_softmax,dim=1)\n",
    "        accuracy = torch.sum(Y==predictions)/float(len(Y))\n",
    "        # compute loss\n",
    "        loss = loss_fnc(output_logits, Y)\n",
    "        # compute gradients\n",
    "        loss.backward()\n",
    "        # update parameters and zero gradients\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        return loss.item(), accuracy*100\n",
    "    return train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nqB698d6VNb5"
   },
   "outputs": [],
   "source": [
    "def make_validate_fnc(model,loss_fnc):\n",
    "    def validate(X,Y):\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            output_logits, output_softmax = model(X)\n",
    "            predictions = torch.argmax(output_softmax,dim=1)\n",
    "            accuracy = torch.sum(Y==predictions)/float(len(Y))\n",
    "            loss = loss_fnc(output_logits,Y)\n",
    "        return loss.item(), accuracy*100, predictions\n",
    "    return validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jxg0uLMGVNb5"
   },
   "source": [
    "stack data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6BRvQ2ajVNb6"
   },
   "outputs": [],
   "source": [
    "X_train = np.stack(mel_train_chunked,axis=0)\n",
    "X_train = np.expand_dims(X_train,2)\n",
    "print('Shape of X_train: ',X_train.shape)\n",
    "X_val = np.stack(mel_val_chunked,axis=0)\n",
    "X_val = np.expand_dims(X_val,2)\n",
    "print('Shape of X_val: ',X_val.shape)\n",
    "X_test = np.stack(mel_test_chunked,axis=0)\n",
    "X_test = np.expand_dims(X_test,2)\n",
    "print('Shape of X_test: ',X_test.shape)\n",
    "\n",
    "del mel_train_chunked\n",
    "del mel_train\n",
    "del mel_val_chunked\n",
    "del mel_val\n",
    "del mel_test_chunked\n",
    "del mel_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vWiObjDXVNb6"
   },
   "source": [
    "scale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ll_nAbOxVNb6"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "b,t,c,h,w = X_train.shape\n",
    "X_train = np.reshape(X_train, newshape=(b,-1))\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_train = np.reshape(X_train, newshape=(b,t,c,h,w))\n",
    "\n",
    "b,t,c,h,w = X_test.shape\n",
    "X_test = np.reshape(X_test, newshape=(b,-1))\n",
    "X_test = scaler.transform(X_test)\n",
    "X_test = np.reshape(X_test, newshape=(b,t,c,h,w))\n",
    "\n",
    "b,t,c,h,w = X_val.shape\n",
    "X_val = np.reshape(X_val, newshape=(b,-1))\n",
    "X_val = scaler.transform(X_val)\n",
    "X_val = np.reshape(X_val, newshape=(b,t,c,h,w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N1WtusHjVNb7"
   },
   "source": [
    "train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TfB3Eo5QVNb7"
   },
   "outputs": [],
   "source": [
    "EPOCHS=700\n",
    "DATASET_SIZE = X_train.shape[0]\n",
    "BATCH_SIZE = 32\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Selected device is {}'.format(device))\n",
    "model = HybridModel(num_emotions=len(EMOTIONS)).to(device)\n",
    "print('Number of trainable params: ',sum(p.numel() for p in model.parameters()) )\n",
    "OPTIMIZER = torch.optim.SGD(model.parameters(),lr=0.01, weight_decay=1e-3, momentum=0.8)\n",
    "\n",
    "train_step = make_train_step(model, loss_fnc, optimizer=OPTIMIZER)\n",
    "validate = make_validate_fnc(model,loss_fnc)\n",
    "losses=[]\n",
    "val_losses = []\n",
    "for epoch in range(EPOCHS):\n",
    "    # schuffle data\n",
    "    ind = np.random.permutation(DATASET_SIZE)\n",
    "    X_train = X_train[ind,:,:,:,:]\n",
    "    Y_train = Y_train[ind]\n",
    "    epoch_acc = 0\n",
    "    epoch_loss = 0\n",
    "    iters = int(DATASET_SIZE / BATCH_SIZE)\n",
    "    for i in range(iters):\n",
    "        batch_start = i * BATCH_SIZE\n",
    "        batch_end = min(batch_start + BATCH_SIZE, DATASET_SIZE)\n",
    "        actual_batch_size = batch_end-batch_start\n",
    "        X = X_train[batch_start:batch_end,:,:,:,:]\n",
    "        Y = Y_train[batch_start:batch_end]\n",
    "        X_tensor = torch.tensor(X,device=device).float()\n",
    "        Y_tensor = torch.tensor(Y, dtype=torch.long,device=device)\n",
    "        loss, acc = train_step(X_tensor,Y_tensor)\n",
    "        epoch_acc += acc*actual_batch_size/DATASET_SIZE\n",
    "        epoch_loss += loss*actual_batch_size/DATASET_SIZE\n",
    "        print(f\"\\r Epoch {epoch}: iteration {i}/{iters}\",end='')\n",
    "    X_val_tensor = torch.tensor(X_val,device=device).float()\n",
    "    Y_val_tensor = torch.tensor(Y_val,dtype=torch.long,device=device)\n",
    "    val_loss, val_acc, _ = validate(X_val_tensor,Y_val_tensor)\n",
    "    losses.append(epoch_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    print('')\n",
    "    print(f\"Epoch {epoch} --> loss:{epoch_loss:.4f}, acc:{epoch_acc:.2f}%, val_loss:{val_loss:.4f}, val_acc:{val_acc:.2f}%\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rN4IKmEkVNb7"
   },
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tn9Vga2ZVNb7"
   },
   "outputs": [],
   "source": [
    "SAVE_PATH = os.path.join(os.getcwd(),'models')\n",
    "os.makedirs('models',exist_ok=True)\n",
    "torch.save(model.state_dict(),os.path.join(SAVE_PATH,'cnn_lstm_model.pt'))\n",
    "print('Model is saved to {}'.format(os.path.join(SAVE_PATH,'cnn_lstm_model.pt')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qApL3Ps0VNb8"
   },
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wk7AyMYeVNb8"
   },
   "outputs": [],
   "source": [
    "LOAD_PATH = os.path.join(os.getcwd(),'models')\n",
    "model = HybridModel(len(EMOTIONS))\n",
    "model.load_state_dict(torch.load(os.path.join(LOAD_PATH,'cnn_lstm_model.pt')))\n",
    "print('Model is loaded from {}'.format(os.path.join(LOAD_PATH,'cnn_lstm_model.pt')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3gd8RoBDVNb8"
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2HGkCDFEVNb8"
   },
   "outputs": [],
   "source": [
    "X_test_tensor = torch.tensor(X_test,device=device).float()\n",
    "Y_test_tensor = torch.tensor(Y_test,dtype=torch.long,device=device)\n",
    "test_loss, test_acc, predictions = validate(X_test_tensor,Y_test_tensor)\n",
    "print(f'Test loss is {test_loss:.3f}')\n",
    "print(f'Test accuracy is {test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8bD9yC9jVNb8"
   },
   "source": [
    "confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dgmYtuKKVNb9"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "\n",
    "predictions = predictions.cpu().numpy()\n",
    "cm = confusion_matrix(Y_test, predictions)\n",
    "names = [EMOTIONS[ind] for ind in range(len(EMOTIONS))]\n",
    "df_cm = pd.DataFrame(cm, index=names, columns=names)\n",
    "# plt.figure(figsize=(10,7))\n",
    "sn.set(font_scale=1.4) # for label size\n",
    "sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}) # font size\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vWGFFPkwVNb9"
   },
   "source": [
    "correlation between emotion intensity and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ny6YkOf0VNb9"
   },
   "outputs": [],
   "source": [
    "correct_strong = 0\n",
    "correct_normal = 0\n",
    "wrong_strong = 0\n",
    "wrong_normal = 0\n",
    "for i in range(len(X_test)):\n",
    "    intensity = data.loc[test_ind[i],'Emotion intensity']\n",
    "    if Y_test[i] == predictions[i]: # correct prediction\n",
    "        if  intensity == 'normal':\n",
    "            correct_normal += 1\n",
    "        else:\n",
    "            correct_strong += 1\n",
    "    else: # wrong prediction\n",
    "        if intensity == 'normal':\n",
    "            wrong_normal += 1\n",
    "        else:\n",
    "            wrong_strong += 1\n",
    "array = np.array([[wrong_normal,wrong_strong],[correct_normal,correct_strong]])\n",
    "df = pd.DataFrame(array,['wrong','correct'],['normal','strong'])\n",
    "sn.set(font_scale=1.4) # for label size\n",
    "sn.heatmap(df, annot=True, annot_kws={\"size\": 16}) # font size\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mKaJpkouVNb9"
   },
   "source": [
    "correlation between gender and corectness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b-GLPzWVVNb-"
   },
   "outputs": [],
   "source": [
    "correct_male = 0\n",
    "correct_female = 0\n",
    "wrong_male = 0\n",
    "wrong_female = 0\n",
    "for i in range(len(X_test)):\n",
    "    gender = data.loc[test_ind[i],'Gender']\n",
    "    if Y_test[i] == predictions[i]: # correct prediction\n",
    "        if  gender == 'male':\n",
    "            correct_male += 1\n",
    "        else:\n",
    "            correct_female += 1\n",
    "    else: # wrong prediction\n",
    "        if gender == 'male':\n",
    "            wrong_male += 1\n",
    "        else:\n",
    "            wrong_female += 1\n",
    "array = np.array([[wrong_male,wrong_female],[correct_male,correct_female]])\n",
    "df = pd.DataFrame(array,['wrong','correct'],['male','female'])\n",
    "sn.set(font_scale=1.4) # for label size\n",
    "sn.heatmap(df, annot=True, annot_kws={\"size\": 16}) # font size\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IkW7JapYVNb-"
   },
   "source": [
    "# Plot loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ntNUHNGEVNb-"
   },
   "outputs": [],
   "source": [
    "plt.plot(losses,'b')\n",
    "plt.plot(val_losses,'r')\n",
    "plt.legend(['train loss','val loss'])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "stacked-cnn-lstm.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
