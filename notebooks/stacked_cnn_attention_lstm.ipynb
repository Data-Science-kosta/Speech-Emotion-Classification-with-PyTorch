{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "szypveleV-5C"
   },
   "source": [
    "# Load file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "id": "WOn1Ce5nV-5G"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import IPython\n",
    "from IPython.display import Audio\n",
    "from IPython.display import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "EMOTIONS = {1:'neutral', 2:'calm', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 0:'surprise'} # surprise je promenjen sa 8 na 0\n",
    "DATA_PATH = '../input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/'\n",
    "SAMPLE_RATE = 48000\n",
    "\n",
    "data = pd.DataFrame(columns=['Emotion', 'Emotion intensity', 'Gender','Path'])\n",
    "for dirname, _, filenames in os.walk(DATA_PATH):\n",
    "    for filename in filenames:\n",
    "        file_path = os.path.join('/kaggle/input/',dirname, filename)\n",
    "        identifiers = filename.split('.')[0].split('-')\n",
    "        emotion = (int(identifiers[2]))\n",
    "        if emotion == 8: # promeni surprise sa 8 na 0\n",
    "            emotion = 0\n",
    "        if int(identifiers[3]) == 1:\n",
    "            emotion_intensity = 'normal' \n",
    "        else:\n",
    "            emotion_intensity = 'strong'\n",
    "        if int(identifiers[6])%2 == 0:\n",
    "            gender = 'female'\n",
    "        else:\n",
    "            gender = 'male'\n",
    "        \n",
    "        data = data.append({\"Emotion\": emotion,\n",
    "                            \"Emotion intensity\": emotion_intensity,\n",
    "                            \"Gender\": gender,\n",
    "                            \"Path\": file_path\n",
    "                             },\n",
    "                             ignore_index = True\n",
    "                          )\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "id": "hgPGPSGmV-5I"
   },
   "outputs": [],
   "source": [
    "print(\"number of files is {}\".format(len(data)))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qPkMcFldV-5K"
   },
   "source": [
    "number of examples per emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4NFsGbASV-5K"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.bar(x=range(8), height=data['Emotion'].value_counts())\n",
    "ax.set_xticks(ticks=range(8))\n",
    "ax.set_xticklabels([EMOTIONS[i] for i in range(8)],fontsize=10)\n",
    "ax.set_xlabel('Emotions')\n",
    "ax.set_ylabel('Number of examples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sN-XzBIMV-5L"
   },
   "source": [
    "number of examples per gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v5K5kcqlV-5L"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "counts = data['Gender'].value_counts()\n",
    "ax.bar(x=[0,1], height=counts.values)\n",
    "ax.set_xticks(ticks=[0,1])\n",
    "ax.set_xticklabels(list(counts.index))\n",
    "ax.set_xlabel('Gender')\n",
    "ax.set_ylabel('Number of examples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EJSMteTfV-5M"
   },
   "source": [
    "number of examples per emotion intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AeMRmiERV-5M"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "counts = data['Emotion intensity'].value_counts()\n",
    "ax.bar(x=[0,1], height=counts.values)\n",
    "ax.set_xticks(ticks=[0,1])\n",
    "ax.set_xticklabels(list(counts.index))\n",
    "ax.set_xlabel('Gender')\n",
    "ax.set_ylabel('Number of examples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iMyKB2JOV-5N"
   },
   "source": [
    "# Load the signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FVDJps4tV-5O"
   },
   "outputs": [],
   "source": [
    "mel_spectrograms = []\n",
    "signals = []\n",
    "for i, file_path in enumerate(data.Path):\n",
    "    audio, sample_rate = librosa.load(file_path, duration=3, offset=0.5, sr=SAMPLE_RATE)\n",
    "    signal = np.zeros((int(SAMPLE_RATE*3,)))\n",
    "    signal[:len(audio)] = audio\n",
    "    signals.append(signal)\n",
    "    print(\"\\r Processed {}/{} files\".format(i,len(data)),end='')\n",
    "signals = np.stack(signals,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b5v34PbWV-5O"
   },
   "source": [
    "# Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pl5ZXsqBV-5O"
   },
   "outputs": [],
   "source": [
    "X = signals\n",
    "train_ind,test_ind,val_ind = [],[],[]\n",
    "X_train,X_val,X_test = [],[],[]\n",
    "Y_train,Y_val,Y_test = [],[],[]\n",
    "for emotion in range(len(EMOTIONS)):\n",
    "    emotion_ind = list(data.loc[data.Emotion==emotion,'Emotion'].index)\n",
    "    emotion_ind = np.random.permutation(emotion_ind)\n",
    "    m = len(emotion_ind)\n",
    "    ind_train = emotion_ind[:int(0.8*m)]\n",
    "    ind_val = emotion_ind[int(0.8*m):int(0.9*m)]\n",
    "    ind_test = emotion_ind[int(0.9*m):]\n",
    "    X_train.append(X[ind_train,:])\n",
    "    Y_train.append(np.array([emotion]*len(ind_train),dtype=np.int32))\n",
    "    X_val.append(X[ind_val,:])\n",
    "    Y_val.append(np.array([emotion]*len(ind_val),dtype=np.int32))\n",
    "    X_test.append(X[ind_test,:])\n",
    "    Y_test.append(np.array([emotion]*len(ind_test),dtype=np.int32))\n",
    "    train_ind.append(ind_train)\n",
    "    test_ind.append(ind_test)\n",
    "    val_ind.append(ind_val)\n",
    "X_train = np.concatenate(X_train,0)\n",
    "X_val = np.concatenate(X_val,0)\n",
    "X_test = np.concatenate(X_test,0)\n",
    "Y_train = np.concatenate(Y_train,0)\n",
    "Y_val = np.concatenate(Y_val,0)\n",
    "Y_test = np.concatenate(Y_test,0)\n",
    "train_ind = np.concatenate(train_ind,0)\n",
    "val_ind = np.concatenate(val_ind,0)\n",
    "test_ind = np.concatenate(test_ind,0)\n",
    "print(f'X_train:{X_train.shape}, Y_train:{Y_train.shape}')\n",
    "print(f'X_val:{X_val.shape}, Y_val:{Y_val.shape}')\n",
    "print(f'X_test:{X_test.shape}, Y_test:{Y_test.shape}')\n",
    "# check if all are unique\n",
    "unique, count = np.unique(np.concatenate([train_ind,test_ind,val_ind],0), return_counts=True)\n",
    "print(\"Number of unique indexes is {}, out of {}\".format(sum(count==1), X.shape[0]))\n",
    "\n",
    "del X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pSXSjh4eV-5Q"
   },
   "source": [
    "# Augment signals by adding AWGN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PQqHDRhsV-5Q"
   },
   "outputs": [],
   "source": [
    "def addAWGN(signal, num_bits=16, augmented_num=2, snr_low=15, snr_high=30): \n",
    "    signal_len = len(signal)\n",
    "    # Generate White Gaussian noise\n",
    "    noise = np.random.normal(size=(augmented_num, signal_len))\n",
    "    # Normalize signal and noise\n",
    "    norm_constant = 2.0**(num_bits-1)\n",
    "    signal_norm = signal / norm_constant\n",
    "    noise_norm = noise / norm_constant\n",
    "    # Compute signal and noise power\n",
    "    s_power = np.sum(signal_norm ** 2) / signal_len\n",
    "    n_power = np.sum(noise_norm ** 2, axis=1) / signal_len\n",
    "    # Random SNR: Uniform [15, 30] in dB\n",
    "    target_snr = np.random.randint(snr_low, snr_high)\n",
    "    # Compute K (covariance matrix) for each noise \n",
    "    K = np.sqrt((s_power / n_power) * 10 ** (- target_snr / 10))\n",
    "    K = np.ones((signal_len, augmented_num)) * K  \n",
    "    # Generate noisy signal\n",
    "    return signal + K.T * noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cV9T6SyYV-5R"
   },
   "outputs": [],
   "source": [
    "aug_signals = []\n",
    "aug_labels = []\n",
    "for i in range(X_train.shape[0]):\n",
    "    signal = X_train[i,:]\n",
    "    augmented_signals = addAWGN(signal)\n",
    "    for j in range(augmented_signals.shape[0]):\n",
    "        aug_labels.append(data.loc[i,\"Emotion\"])\n",
    "        aug_signals.append(augmented_signals[j,:])\n",
    "        data = data.append(data.iloc[i], ignore_index=True)\n",
    "    print(\"\\r Processed {}/{} files\".format(i,X_train.shape[0]),end='')\n",
    "aug_signals = np.stack(aug_signals,axis=0)\n",
    "X_train = np.concatenate([X_train,aug_signals],axis=0)\n",
    "aug_labels = np.stack(aug_labels,axis=0)\n",
    "Y_train = np.concatenate([Y_train,aug_labels])\n",
    "print('')\n",
    "print(f'X_train:{X_train.shape}, Y_train:{Y_train.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MGW-G_tBV-5S"
   },
   "source": [
    "# Calculate mel spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3HxpDFl5V-5S"
   },
   "outputs": [],
   "source": [
    "def getMELspectrogram(audio, sample_rate):\n",
    "    mel_spec = librosa.feature.melspectrogram(y=audio,\n",
    "                                              sr=sample_rate,\n",
    "                                              n_fft=1024,\n",
    "                                              win_length = 512,\n",
    "                                              window='hamming',\n",
    "                                              hop_length = 256,\n",
    "                                              n_mels=128,\n",
    "                                              fmax=sample_rate/2\n",
    "                                             )\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    return mel_spec_db\n",
    "\n",
    "# test function\n",
    "audio, sample_rate = librosa.load(data.loc[0,'Path'], duration=3, offset=0.5,sr=SAMPLE_RATE)\n",
    "signal = np.zeros((int(SAMPLE_RATE*3,)))\n",
    "signal[:len(audio)] = audio\n",
    "mel_spectrogram = getMELspectrogram(signal, SAMPLE_RATE)\n",
    "librosa.display.specshow(mel_spectrogram, y_axis='mel', x_axis='time')\n",
    "print('MEL spectrogram shape: ',mel_spectrogram.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v6a5OtrdV-5T"
   },
   "outputs": [],
   "source": [
    "mel_train = []\n",
    "print(\"Calculatin mel spectrograms for train set\")\n",
    "for i in range(X_train.shape[0]):\n",
    "    mel_spectrogram = getMELspectrogram(X_train[i,:], sample_rate=SAMPLE_RATE)\n",
    "    mel_train.append(mel_spectrogram)\n",
    "    print(\"\\r Processed {}/{} files\".format(i,X_train.shape[0]),end='')\n",
    "print('')\n",
    "del X_train\n",
    "\n",
    "mel_val = []\n",
    "print(\"Calculatin mel spectrograms for val set\")\n",
    "for i in range(X_val.shape[0]):\n",
    "    mel_spectrogram = getMELspectrogram(X_val[i,:], sample_rate=SAMPLE_RATE)\n",
    "    mel_val.append(mel_spectrogram)\n",
    "    print(\"\\r Processed {}/{} files\".format(i,X_val.shape[0]),end='')\n",
    "print('')\n",
    "del X_val\n",
    "\n",
    "mel_test = []\n",
    "print(\"Calculatin mel spectrograms for test set\")\n",
    "for i in range(X_test.shape[0]):\n",
    "    mel_spectrogram = getMELspectrogram(X_test[i,:], sample_rate=SAMPLE_RATE)\n",
    "    mel_test.append(mel_spectrogram)\n",
    "    print(\"\\r Processed {}/{} files\".format(i,X_test.shape[0]),end='')\n",
    "print('')\n",
    "del X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9aco3oRiV-5T"
   },
   "source": [
    "# Split into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x1f1Nj3uV-5U"
   },
   "outputs": [],
   "source": [
    "def splitIntoChunks(mel_spec,win_size,stride):\n",
    "    t = mel_spec.shape[1]\n",
    "    num_of_chunks = int(t/stride)\n",
    "    chunks = []\n",
    "    for i in range(num_of_chunks):\n",
    "        chunk = mel_spec[:,i*stride:i*stride+win_size]\n",
    "        if chunk.shape[1] == win_size:\n",
    "            chunks.append(chunk)\n",
    "    return np.stack(chunks,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UVC0P8YmV-5U"
   },
   "outputs": [],
   "source": [
    "# get chunks\n",
    "# train set\n",
    "mel_train_chunked = []\n",
    "for mel_spec in mel_train:\n",
    "    chunks = splitIntoChunks(mel_spec, win_size=128,stride=64)\n",
    "    mel_train_chunked.append(chunks)\n",
    "print(\"Number of chunks is {}\".format(chunks.shape[0]))\n",
    "# val set\n",
    "mel_val_chunked = []\n",
    "for mel_spec in mel_val:\n",
    "    chunks = splitIntoChunks(mel_spec, win_size=128,stride=64)\n",
    "    mel_val_chunked.append(chunks)\n",
    "print(\"Number of chunks is {}\".format(chunks.shape[0]))\n",
    "# test set\n",
    "mel_test_chunked = []\n",
    "for mel_spec in mel_test:\n",
    "    chunks = splitIntoChunks(mel_spec, win_size=128,stride=64)\n",
    "    mel_test_chunked.append(chunks)\n",
    "print(\"Number of chunks is {}\".format(chunks.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NrTLj4wcV-5V"
   },
   "source": [
    "# Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1EEHbeCKV-5W"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# BATCH FIRST TimeDistributed layer\n",
    "class TimeDistributed(nn.Module):\n",
    "    def __init__(self, module):\n",
    "        super(TimeDistributed, self).__init__()\n",
    "        self.module = module\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if len(x.size()) <= 2:\n",
    "            return self.module(x)\n",
    "        # squash samples and timesteps into a single axis\n",
    "        elif len(x.size()) == 3: # (samples, timesteps, inp1)\n",
    "            x_reshape = x.contiguous().view(-1, x.size(2))  # (samples * timesteps, inp1)\n",
    "        elif len(x.size()) == 4: # (samples,timesteps,inp1,inp2)\n",
    "            x_reshape = x.contiguous().view(-1, x.size(2), x.size(3)) # (samples*timesteps,inp1,inp2)\n",
    "        else: # (samples,timesteps,inp1,inp2,inp3)\n",
    "            x_reshape = x.contiguous().view(-1, x.size(2), x.size(3),x.size(4)) # (samples*timesteps,inp1,inp2,inp3)\n",
    "            \n",
    "        y = self.module(x_reshape)\n",
    "        \n",
    "        # we have to reshape Y\n",
    "        if len(x.size()) == 3:\n",
    "            y = y.contiguous().view(x.size(0), -1, y.size(1))  # (samples, timesteps, out1)\n",
    "        elif len(x.size()) == 4:\n",
    "            y = y.contiguous().view(x.size(0), -1, y.size(1), y.size(2)) # (samples, timesteps, out1,out2)\n",
    "        else:\n",
    "            y = y.contiguous().view(x.size(0), -1, y.size(1), y.size(2),y.size(3)) # (samples, timesteps, out1,out2, out3)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GxKDjc53V-5W"
   },
   "outputs": [],
   "source": [
    "class HybridModel(nn.Module):\n",
    "    def __init__(self,num_emotions):\n",
    "        super().__init__()\n",
    "        # conv block\n",
    "        self.conv2Dblock = nn.Sequential(\n",
    "            # 1. conv block\n",
    "            TimeDistributed(nn.Conv2d(in_channels=1,\n",
    "                                   out_channels=16,\n",
    "                                   kernel_size=3,\n",
    "                                   stride=1,\n",
    "                                   padding=1\n",
    "                                  )),\n",
    "            TimeDistributed(nn.BatchNorm2d(16)),\n",
    "            TimeDistributed(nn.ReLU()),\n",
    "            TimeDistributed(nn.MaxPool2d(kernel_size=2, stride=2)),\n",
    "            TimeDistributed(nn.Dropout(p=0.3)),\n",
    "            # 2. conv block\n",
    "            TimeDistributed(nn.Conv2d(in_channels=16,\n",
    "                                   out_channels=32,\n",
    "                                   kernel_size=3,\n",
    "                                   stride=1,\n",
    "                                   padding=1\n",
    "                                  )),\n",
    "            TimeDistributed(nn.BatchNorm2d(32)),\n",
    "            TimeDistributed(nn.ReLU()),\n",
    "            TimeDistributed(nn.MaxPool2d(kernel_size=4, stride=4)),\n",
    "            TimeDistributed(nn.Dropout(p=0.3)),\n",
    "            # 3. conv block\n",
    "            TimeDistributed(nn.Conv2d(in_channels=32,\n",
    "                                   out_channels=64,\n",
    "                                   kernel_size=3,\n",
    "                                   stride=1,\n",
    "                                   padding=1\n",
    "                                  )),\n",
    "            TimeDistributed(nn.BatchNorm2d(64)),\n",
    "            TimeDistributed(nn.ReLU()),\n",
    "            TimeDistributed(nn.MaxPool2d(kernel_size=4, stride=4)),\n",
    "            TimeDistributed(nn.Dropout(p=0.3))\n",
    "        )\n",
    "        # LSTM block\n",
    "        hidden_size = 64\n",
    "        self.lstm = nn.LSTM(input_size=1024,hidden_size=hidden_size,bidirectional=True, batch_first=True)\n",
    "        self.dropout_lstm = nn.Dropout(p=0.4)\n",
    "        self.attention_linear = nn.Linear(2*hidden_size,1) # 2*hidden_size for the 2 outputs of bidir LSTM\n",
    "        # Linear softmax layer\n",
    "        self.out_linear = nn.Linear(2*hidden_size,num_emotions)\n",
    "    def forward(self,x):\n",
    "        conv_embedding = self.conv2Dblock(x)\n",
    "        conv_embedding = torch.flatten(conv_embedding, start_dim=2) # do not flatten batch dimension and time\n",
    "        lstm_embedding, (h,c) = self.lstm(conv_embedding)\n",
    "        lstm_embedding = self.dropout_lstm(lstm_embedding)\n",
    "        # lstm_embedding (batch, time, hidden_size*2)\n",
    "        batch_size,T,_ = lstm_embedding.shape \n",
    "        attention_weights = [None]*T\n",
    "        for t in range(T):\n",
    "            embedding = lstm_embedding[:,t,:]\n",
    "            attention_weights[t] = self.attention_linear(embedding)\n",
    "        attention_weights_norm = nn.functional.softmax(torch.stack(attention_weights,-1),dim=-1)\n",
    "        attention = torch.bmm(attention_weights_norm,lstm_embedding) # (Bx1xT)*(B,T,hidden_size*2)=(B,1,2*hidden_size)\n",
    "        attention = torch.squeeze(attention, 1)\n",
    "        output_logits = self.out_linear(attention)\n",
    "        output_softmax = nn.functional.softmax(output_logits,dim=1)\n",
    "        return output_logits, output_softmax, attention_weights_norm\n",
    "                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5yY_NDcmV-5X"
   },
   "outputs": [],
   "source": [
    "def loss_fnc(predictions, targets):\n",
    "    return nn.CrossEntropyLoss()(input=predictions,target=targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KgtvBhgPV-5X"
   },
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2MlGDx27V-5Y"
   },
   "outputs": [],
   "source": [
    "def make_train_step(model, loss_fnc, optimizer):\n",
    "    def train_step(X,Y):\n",
    "        # set model to train mode\n",
    "        model.train()\n",
    "        # forward pass\n",
    "        output_logits, output_softmax, attention_weights_norm = model(X)\n",
    "        predictions = torch.argmax(output_softmax,dim=1)\n",
    "        accuracy = torch.sum(Y==predictions)/float(len(Y))\n",
    "        # compute loss\n",
    "        loss = loss_fnc(output_logits, Y)\n",
    "        # compute gradients\n",
    "        loss.backward()\n",
    "        # update parameters and zero gradients\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        return loss.item(), accuracy*100\n",
    "    return train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O_ERE1zsV-5Y"
   },
   "outputs": [],
   "source": [
    "def make_validate_fnc(model,loss_fnc):\n",
    "    def validate(X,Y):\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            output_logits, output_softmax, attention_weights_norm = model(X)\n",
    "            predictions = torch.argmax(output_softmax,dim=1)\n",
    "            accuracy = torch.sum(Y==predictions)/float(len(Y))\n",
    "            loss = loss_fnc(output_logits,Y)\n",
    "        return loss.item(), accuracy*100, predictions\n",
    "    return validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mc3aH9A2V-5Y"
   },
   "source": [
    "stack data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QKbcznwkV-5Z"
   },
   "outputs": [],
   "source": [
    "X_train = np.stack(mel_train_chunked,axis=0)\n",
    "X_train = np.expand_dims(X_train,2)\n",
    "print('Shape of X_train: ',X_train.shape)\n",
    "X_val = np.stack(mel_val_chunked,axis=0)\n",
    "X_val = np.expand_dims(X_val,2)\n",
    "print('Shape of X_val: ',X_val.shape)\n",
    "X_test = np.stack(mel_test_chunked,axis=0)\n",
    "X_test = np.expand_dims(X_test,2)\n",
    "print('Shape of X_test: ',X_test.shape)\n",
    "\n",
    "del mel_train_chunked\n",
    "del mel_train\n",
    "del mel_val_chunked\n",
    "del mel_val\n",
    "del mel_test_chunked\n",
    "del mel_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DhsBgPORV-5Z"
   },
   "source": [
    "scale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tDvt4busV-5a"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "b,t,c,h,w = X_train.shape\n",
    "X_train = np.reshape(X_train, newshape=(b,-1))\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_train = np.reshape(X_train, newshape=(b,t,c,h,w))\n",
    "\n",
    "b,t,c,h,w = X_test.shape\n",
    "X_test = np.reshape(X_test, newshape=(b,-1))\n",
    "X_test = scaler.transform(X_test)\n",
    "X_test = np.reshape(X_test, newshape=(b,t,c,h,w))\n",
    "\n",
    "b,t,c,h,w = X_val.shape\n",
    "X_val = np.reshape(X_val, newshape=(b,-1))\n",
    "X_val = scaler.transform(X_val)\n",
    "X_val = np.reshape(X_val, newshape=(b,t,c,h,w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZObh6dQ6V-5a"
   },
   "source": [
    "train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0q-9UChKV-5a"
   },
   "outputs": [],
   "source": [
    "EPOCHS=200\n",
    "DATASET_SIZE = X_train.shape[0]\n",
    "BATCH_SIZE = 32\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Selected device is {}'.format(device))\n",
    "model = HybridModel(num_emotions=len(EMOTIONS)).to(device)\n",
    "print('Number of trainable params: ',sum(p.numel() for p in model.parameters()))\n",
    "OPTIMIZER = torch.optim.SGD(model.parameters(),lr=0.01, weight_decay=1e-3, momentum=0.8)\n",
    "\n",
    "train_step = make_train_step(model, loss_fnc, optimizer=OPTIMIZER)\n",
    "validate = make_validate_fnc(model,loss_fnc)\n",
    "losses=[]\n",
    "val_losses = []\n",
    "for epoch in range(EPOCHS):\n",
    "    # schuffle data\n",
    "    ind = np.random.permutation(DATASET_SIZE)\n",
    "    X_train = X_train[ind,:,:,:,:]\n",
    "    Y_train = Y_train[ind]\n",
    "    epoch_acc = 0\n",
    "    epoch_loss = 0\n",
    "    iters = int(DATASET_SIZE / BATCH_SIZE)\n",
    "    for i in range(iters):\n",
    "        batch_start = i * BATCH_SIZE\n",
    "        batch_end = min(batch_start + BATCH_SIZE, DATASET_SIZE)\n",
    "        actual_batch_size = batch_end-batch_start\n",
    "        X = X_train[batch_start:batch_end,:,:,:,:]\n",
    "        Y = Y_train[batch_start:batch_end]\n",
    "        X_tensor = torch.tensor(X,device=device).float()\n",
    "        Y_tensor = torch.tensor(Y, dtype=torch.long,device=device)\n",
    "        loss, acc = train_step(X_tensor,Y_tensor)\n",
    "        epoch_acc += acc*actual_batch_size/DATASET_SIZE\n",
    "        epoch_loss += loss*actual_batch_size/DATASET_SIZE\n",
    "        print(f\"\\r Epoch {epoch}: iteration {i}/{iters}\",end='')\n",
    "    X_val_tensor = torch.tensor(X_val,device=device).float()\n",
    "    Y_val_tensor = torch.tensor(Y_val,dtype=torch.long,device=device)\n",
    "    val_loss, val_acc, _ = validate(X_val_tensor,Y_val_tensor)\n",
    "    losses.append(epoch_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    print('')\n",
    "    print(f\"Epoch {epoch} --> loss:{epoch_loss:.4f}, acc:{epoch_acc:.2f}%, val_loss:{val_loss:.4f}, val_acc:{val_acc:.2f}%\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xs5nF_rdV-5b"
   },
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "knnRTMTUV-5b"
   },
   "outputs": [],
   "source": [
    "SAVE_PATH = os.path.join(os.getcwd(),'models')\n",
    "os.makedirs('models',exist_ok=True)\n",
    "torch.save(model.state_dict(),os.path.join(SAVE_PATH,'cnn_attention_lstm_model.pt'))\n",
    "print('Model is saved to {}'.format(os.path.join(SAVE_PATH,'cnn_attention_lstm_model.pt')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ig13hm-VV-5b"
   },
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0yn7KjRLV-5b"
   },
   "outputs": [],
   "source": [
    "LOAD_PATH = os.path.join(os.getcwd(),'models')\n",
    "model = HybridModel(len(EMOTIONS))\n",
    "model.load_state_dict(torch.load(os.path.join(LOAD_PATH,'cnn_attention_lstm_model.pt')))\n",
    "print('Model is loaded from {}'.format(os.path.join(LOAD_PATH,'cnn_attention_lstm_model.pt')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PchoyndTV-5f"
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4-ab0ee4V-5g"
   },
   "outputs": [],
   "source": [
    "X_test_tensor = torch.tensor(X_test,device=device).float()\n",
    "Y_test_tensor = torch.tensor(Y_test,dtype=torch.long,device=device)\n",
    "test_loss, test_acc, predictions = validate(X_test_tensor,Y_test_tensor)\n",
    "print(f'Test loss is {test_loss:.3f}')\n",
    "print(f'Test accuracy is {test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o0Wk2hPoV-5g"
   },
   "source": [
    "confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "njzkjPTDV-5h"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "\n",
    "predictions = predictions.cpu().numpy()\n",
    "cm = confusion_matrix(Y_test, predictions)\n",
    "names = [EMOTIONS[ind] for ind in range(len(EMOTIONS))]\n",
    "df_cm = pd.DataFrame(cm, index=names, columns=names)\n",
    "# plt.figure(figsize=(10,7))\n",
    "sn.set(font_scale=1.4) # for label size\n",
    "sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}) # font size\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hY0zi7z5V-5h"
   },
   "source": [
    "Correlation between emotion intensity and correct prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JYOv8bbLV-5h"
   },
   "outputs": [],
   "source": [
    "correct_strong = 0\n",
    "correct_normal = 0\n",
    "wrong_strong = 0\n",
    "wrong_normal = 0\n",
    "for i in range(len(X_test)):\n",
    "    intensity = data.loc[test_ind[i],'Emotion intensity']\n",
    "    if Y_test[i] == predictions[i]: # correct prediction\n",
    "        if  intensity == 'normal':\n",
    "            correct_normal += 1\n",
    "        else:\n",
    "            correct_strong += 1\n",
    "    else: # wrong prediction\n",
    "        if intensity == 'normal':\n",
    "            wrong_normal += 1\n",
    "        else:\n",
    "            wrong_strong += 1\n",
    "array = np.array([[wrong_normal,wrong_strong],[correct_normal,correct_strong]])\n",
    "df = pd.DataFrame(array,['wrong','correct'],['normal','strong'])\n",
    "sn.set(font_scale=1.4) # for label size\n",
    "sn.heatmap(df, annot=True, annot_kws={\"size\": 16}) # font size\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UqN73dUHV-5h"
   },
   "source": [
    "correlation between gender and corectness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "otBTHGrVV-5i"
   },
   "outputs": [],
   "source": [
    "correct_male = 0\n",
    "correct_female = 0\n",
    "wrong_male = 0\n",
    "wrong_female = 0\n",
    "for i in range(len(X_test)):\n",
    "    gender = data.loc[test_ind[i],'Gender']\n",
    "    if Y_test[i] == predictions[i]: # correct prediction\n",
    "        if  gender == 'male':\n",
    "            correct_male += 1\n",
    "        else:\n",
    "            correct_female += 1\n",
    "    else: # wrong prediction\n",
    "        if gender == 'male':\n",
    "            wrong_male += 1\n",
    "        else:\n",
    "            wrong_female += 1\n",
    "array = np.array([[wrong_male,wrong_female],[correct_male,correct_female]])\n",
    "df = pd.DataFrame(array,['wrong','correct'],['male','female'])\n",
    "sn.set(font_scale=1.4) # for label size\n",
    "sn.heatmap(df, annot=True, annot_kws={\"size\": 16}) # font size\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WiK0-Np5V-5i"
   },
   "source": [
    "# Plot loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N0iSMb1CV-5i"
   },
   "outputs": [],
   "source": [
    "plt.plot(losses,'b')\n",
    "plt.plot(val_losses,'r')\n",
    "plt.legend(['train loss','val loss'])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "stacked-cnn-attention-lstm.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
