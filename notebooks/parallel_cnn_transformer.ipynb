{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8fYFLFMFXSZk"
   },
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "id": "2CvOXDYwXSZn"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries install|ed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import IPython\n",
    "from IPython.display import Audio\n",
    "from IPython.display import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "EMOTIONS = {1:'neutral', 2:'calm', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 0:'surprise'} # surprise je promenjen sa 8 na 0\n",
    "DATA_PATH = '../input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/'\n",
    "SAMPLE_RATE = 48000\n",
    "\n",
    "data = pd.DataFrame(columns=['Emotion', 'Emotion intensity', 'Gender','Path'])\n",
    "for dirname, _, filenames in os.walk(DATA_PATH):\n",
    "    for filename in filenames:\n",
    "        file_path = os.path.join('/kaggle/input/',dirname, filename)\n",
    "        identifiers = filename.split('.')[0].split('-')\n",
    "        emotion = (int(identifiers[2]))\n",
    "        if emotion == 8: # promeni surprise sa 8 na 0\n",
    "            emotion = 0\n",
    "        if int(identifiers[3]) == 1:\n",
    "            emotion_intensity = 'normal' \n",
    "        else:\n",
    "            emotion_intensity = 'strong'\n",
    "        if int(identifiers[6])%2 == 0:\n",
    "            gender = 'female'\n",
    "        else:\n",
    "            gender = 'male'\n",
    "        \n",
    "        data = data.append({\"Emotion\": emotion,\n",
    "                            \"Emotion intensity\": emotion_intensity,\n",
    "                            \"Gender\": gender,\n",
    "                            \"Path\": file_path\n",
    "                             },\n",
    "                             ignore_index = True\n",
    "                          )\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "id": "RNWLXz86XSZp"
   },
   "outputs": [],
   "source": [
    "print(\"number of files is {}\".format(len(data)))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OIAAwt2ZXSZp"
   },
   "source": [
    "Number of examples per emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1idtahlZXSZq"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.bar(x=range(8), height=data['Emotion'].value_counts())\n",
    "ax.set_xticks(ticks=range(8))\n",
    "ax.set_xticklabels([EMOTIONS[i] for i in range(8)],fontsize=10)\n",
    "ax.set_xlabel('Emotions')\n",
    "ax.set_ylabel('Number of examples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3BaFvI-bXSZr"
   },
   "source": [
    "number of examples per gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WuhgrwJVXSZs"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "counts = data['Gender'].value_counts()\n",
    "ax.bar(x=[0,1], height=counts.values)\n",
    "ax.set_xticks(ticks=[0,1])\n",
    "ax.set_xticklabels(list(counts.index))\n",
    "ax.set_xlabel('Gender')\n",
    "ax.set_ylabel('Number of examples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZHdXgdNWXSZt"
   },
   "source": [
    "number of examples per emotion intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RoqLDuYqXSZu"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "counts = data['Emotion intensity'].value_counts()\n",
    "ax.bar(x=[0,1], height=counts.values)\n",
    "ax.set_xticks(ticks=[0,1])\n",
    "ax.set_xticklabels(list(counts.index))\n",
    "ax.set_xlabel('Gender')\n",
    "ax.set_ylabel('Number of examples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5inOANfJXSZv"
   },
   "source": [
    "# Load the signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vmGZx4HpXSZw"
   },
   "outputs": [],
   "source": [
    "mel_spectrograms = []\n",
    "signals = []\n",
    "for i, file_path in enumerate(data.Path):\n",
    "    audio, sample_rate = librosa.load(file_path, duration=3, offset=0.5, sr=SAMPLE_RATE)\n",
    "    signal = np.zeros((int(SAMPLE_RATE*3,)))\n",
    "    signal[:len(audio)] = audio\n",
    "    signals.append(signal)\n",
    "    print(\"\\r Processed {}/{} files\".format(i,len(data)),end='')\n",
    "signals = np.stack(signals,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UgakfUHrXSZx"
   },
   "source": [
    "# Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IUsQeYo0XSZy"
   },
   "outputs": [],
   "source": [
    "X = signals\n",
    "train_ind,test_ind,val_ind = [],[],[]\n",
    "X_train,X_val,X_test = [],[],[]\n",
    "Y_train,Y_val,Y_test = [],[],[]\n",
    "for emotion in range(len(EMOTIONS)):\n",
    "    emotion_ind = list(data.loc[data.Emotion==emotion,'Emotion'].index)\n",
    "    emotion_ind = np.random.permutation(emotion_ind)\n",
    "    m = len(emotion_ind)\n",
    "    ind_train = emotion_ind[:int(0.8*m)]\n",
    "    ind_val = emotion_ind[int(0.8*m):int(0.9*m)]\n",
    "    ind_test = emotion_ind[int(0.9*m):]\n",
    "    X_train.append(X[ind_train,:])\n",
    "    Y_train.append(np.array([emotion]*len(ind_train),dtype=np.int32))\n",
    "    X_val.append(X[ind_val,:])\n",
    "    Y_val.append(np.array([emotion]*len(ind_val),dtype=np.int32))\n",
    "    X_test.append(X[ind_test,:])\n",
    "    Y_test.append(np.array([emotion]*len(ind_test),dtype=np.int32))\n",
    "    train_ind.append(ind_train)\n",
    "    test_ind.append(ind_test)\n",
    "    val_ind.append(ind_val)\n",
    "X_train = np.concatenate(X_train,0)\n",
    "X_val = np.concatenate(X_val,0)\n",
    "X_test = np.concatenate(X_test,0)\n",
    "Y_train = np.concatenate(Y_train,0)\n",
    "Y_val = np.concatenate(Y_val,0)\n",
    "Y_test = np.concatenate(Y_test,0)\n",
    "train_ind = np.concatenate(train_ind,0)\n",
    "val_ind = np.concatenate(val_ind,0)\n",
    "test_ind = np.concatenate(test_ind,0)\n",
    "print(f'X_train:{X_train.shape}, Y_train:{Y_train.shape}')\n",
    "print(f'X_val:{X_val.shape}, Y_val:{Y_val.shape}')\n",
    "print(f'X_test:{X_test.shape}, Y_test:{Y_test.shape}')\n",
    "# check if all are unique\n",
    "unique, count = np.unique(np.concatenate([train_ind,test_ind,val_ind],0), return_counts=True)\n",
    "print(\"Number of unique indexes is {}, out of {}\".format(sum(count==1), X.shape[0]))\n",
    "\n",
    "del X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aJYB_22fXSZy"
   },
   "source": [
    "# Augment signals by adding AWGN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ud5owNWTXSZz"
   },
   "outputs": [],
   "source": [
    "def addAWGN(signal, num_bits=16, augmented_num=2, snr_low=15, snr_high=30): \n",
    "    signal_len = len(signal)\n",
    "    # Generate White Gaussian noise\n",
    "    noise = np.random.normal(size=(augmented_num, signal_len))\n",
    "    # Normalize signal and noise\n",
    "    norm_constant = 2.0**(num_bits-1)\n",
    "    signal_norm = signal / norm_constant\n",
    "    noise_norm = noise / norm_constant\n",
    "    # Compute signal and noise power\n",
    "    s_power = np.sum(signal_norm ** 2) / signal_len\n",
    "    n_power = np.sum(noise_norm ** 2, axis=1) / signal_len\n",
    "    # Random SNR: Uniform [15, 30] in dB\n",
    "    target_snr = np.random.randint(snr_low, snr_high)\n",
    "    # Compute K (covariance matrix) for each noise \n",
    "    K = np.sqrt((s_power / n_power) * 10 ** (- target_snr / 10))\n",
    "    K = np.ones((signal_len, augmented_num)) * K  \n",
    "    # Generate noisy signal\n",
    "    return signal + K.T * noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gGrnLiBdXSZz"
   },
   "outputs": [],
   "source": [
    "aug_signals = []\n",
    "aug_labels = []\n",
    "for i in range(X_train.shape[0]):\n",
    "    signal = X_train[i,:]\n",
    "    augmented_signals = addAWGN(signal)\n",
    "    for j in range(augmented_signals.shape[0]):\n",
    "        aug_labels.append(data.loc[i,\"Emotion\"])\n",
    "        aug_signals.append(augmented_signals[j,:])\n",
    "        data = data.append(data.iloc[i], ignore_index=True)\n",
    "    print(\"\\r Processed {}/{} files\".format(i,X_train.shape[0]),end='')\n",
    "aug_signals = np.stack(aug_signals,axis=0)\n",
    "X_train = np.concatenate([X_train,aug_signals],axis=0)\n",
    "aug_labels = np.stack(aug_labels,axis=0)\n",
    "Y_train = np.concatenate([Y_train,aug_labels])\n",
    "print('')\n",
    "print(f'X_train:{X_train.shape}, Y_train:{Y_train.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sd_2Xfi-XSZ1"
   },
   "source": [
    "# Calculate mel spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oFpJgmgGXSZ1"
   },
   "outputs": [],
   "source": [
    "def getMELspectrogram(audio, sample_rate):\n",
    "    mel_spec = librosa.feature.melspectrogram(y=audio,\n",
    "                                              sr=sample_rate,\n",
    "                                              n_fft=1024,\n",
    "                                              win_length = 512,\n",
    "                                              window='hamming',\n",
    "                                              hop_length = 256,\n",
    "                                              n_mels=128,\n",
    "                                              fmax=sample_rate/2\n",
    "                                             )\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    return mel_spec_db\n",
    "\n",
    "# test function\n",
    "audio, sample_rate = librosa.load(data.loc[0,'Path'], duration=3, offset=0.5,sr=SAMPLE_RATE)\n",
    "signal = np.zeros((int(SAMPLE_RATE*3,)))\n",
    "signal[:len(audio)] = audio\n",
    "mel_spectrogram = getMELspectrogram(signal, SAMPLE_RATE)\n",
    "librosa.display.specshow(mel_spectrogram, y_axis='mel', x_axis='time')\n",
    "print('MEL spectrogram shape: ',mel_spectrogram.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LrAaTRJIXSZ2"
   },
   "outputs": [],
   "source": [
    "mel_train = []\n",
    "print(\"Calculatin mel spectrograms for train set\")\n",
    "for i in range(X_train.shape[0]):\n",
    "    mel_spectrogram = getMELspectrogram(X_train[i,:], sample_rate=SAMPLE_RATE)\n",
    "    mel_train.append(mel_spectrogram)\n",
    "    print(\"\\r Processed {}/{} files\".format(i,X_train.shape[0]),end='')\n",
    "print('')\n",
    "mel_train = np.stack(mel_train,axis=0)\n",
    "del X_train\n",
    "X_train = mel_train\n",
    "\n",
    "mel_val = []\n",
    "print(\"Calculatin mel spectrograms for val set\")\n",
    "for i in range(X_val.shape[0]):\n",
    "    mel_spectrogram = getMELspectrogram(X_val[i,:], sample_rate=SAMPLE_RATE)\n",
    "    mel_val.append(mel_spectrogram)\n",
    "    print(\"\\r Processed {}/{} files\".format(i,X_val.shape[0]),end='')\n",
    "print('')\n",
    "mel_val = np.stack(mel_val,axis=0)\n",
    "del X_val\n",
    "X_val = mel_val\n",
    "\n",
    "mel_test = []\n",
    "print(\"Calculatin mel spectrograms for test set\")\n",
    "for i in range(X_test.shape[0]):\n",
    "    mel_spectrogram = getMELspectrogram(X_test[i,:], sample_rate=SAMPLE_RATE)\n",
    "    mel_test.append(mel_spectrogram)\n",
    "    print(\"\\r Processed {}/{} files\".format(i,X_test.shape[0]),end='')\n",
    "print('')\n",
    "mel_test = np.stack(mel_test,axis=0)\n",
    "del X_test\n",
    "X_test = mel_test\n",
    "\n",
    "print(f'X_train:{X_train.shape}, Y_train:{Y_train.shape}')\n",
    "print(f'X_val:{X_val.shape}, Y_val:{Y_val.shape}')\n",
    "print(f'X_test:{X_test.shape}, Y_test:{Y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "msKiHlDSXSZ3"
   },
   "source": [
    "# Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dIX_bfpcXSZ3"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ParallelModel(nn.Module):\n",
    "    def __init__(self,num_emotions):\n",
    "        super().__init__()\n",
    "        # conv block\n",
    "        self.conv2Dblock = nn.Sequential(\n",
    "            # 1. conv block\n",
    "            nn.Conv2d(in_channels=1,\n",
    "                       out_channels=16,\n",
    "                       kernel_size=3,\n",
    "                       stride=1,\n",
    "                       padding=1\n",
    "                      ),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(p=0.3),\n",
    "            # 2. conv block\n",
    "            nn.Conv2d(in_channels=16,\n",
    "                       out_channels=32,\n",
    "                       kernel_size=3,\n",
    "                       stride=1,\n",
    "                       padding=1\n",
    "                      ),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=4, stride=4),\n",
    "            nn.Dropout(p=0.3),\n",
    "            # 3. conv block\n",
    "            nn.Conv2d(in_channels=32,\n",
    "                       out_channels=64,\n",
    "                       kernel_size=3,\n",
    "                       stride=1,\n",
    "                       padding=1\n",
    "                      ),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=4, stride=4),\n",
    "            nn.Dropout(p=0.3),\n",
    "            # 4. conv block\n",
    "            nn.Conv2d(in_channels=64,\n",
    "                       out_channels=64,\n",
    "                       kernel_size=3,\n",
    "                       stride=1,\n",
    "                       padding=1\n",
    "                      ),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=4, stride=4),\n",
    "            nn.Dropout(p=0.3)\n",
    "        )\n",
    "        # Transformer block\n",
    "        self.transf_maxpool = nn.MaxPool2d(kernel_size=[2,4], stride=[2,4])\n",
    "        transf_layer = nn.TransformerEncoderLayer(d_model=64, nhead=4, dim_feedforward=512, dropout=0.4, activation='relu')\n",
    "        self.transf_encoder = nn.TransformerEncoder(transf_layer, num_layers=4)\n",
    "        # Linear softmax layer\n",
    "        self.out_linear = nn.Linear(320,num_emotions)\n",
    "        self.dropout_linear = nn.Dropout(p=0)\n",
    "        self.out_softmax = nn.Softmax(dim=1)\n",
    "    def forward(self,x):\n",
    "        # conv embedding\n",
    "        conv_embedding = self.conv2Dblock(x) #(b,channel,freq,time)\n",
    "        conv_embedding = torch.flatten(conv_embedding, start_dim=1) # do not flatten batch dimension\n",
    "        # transformer embedding\n",
    "        x_reduced = self.transf_maxpool(x)\n",
    "        x_reduced = torch.squeeze(x_reduced,1)\n",
    "        x_reduced = x_reduced.permute(2,0,1) # requires shape = (time,batch,embedding)\n",
    "        transf_out = self.transf_encoder(x_reduced)\n",
    "        transf_embedding = torch.mean(transf_out, dim=0)\n",
    "        # concatenate\n",
    "        complete_embedding = torch.cat([conv_embedding, transf_embedding], dim=1) \n",
    "        # final Linear\n",
    "        output_logits = self.out_linear(complete_embedding)\n",
    "        output_logits = self.dropout_linear(output_logits)\n",
    "        output_softmax = self.out_softmax(output_logits)\n",
    "        return output_logits, output_softmax\n",
    "                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-B8BF95qXSZ4"
   },
   "outputs": [],
   "source": [
    "def loss_fnc(predictions, targets):\n",
    "    return nn.CrossEntropyLoss()(input=predictions,target=targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EwCWQ3qgXSZ5"
   },
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kG-QAU39XSZ5"
   },
   "outputs": [],
   "source": [
    "def make_train_step(model, loss_fnc, optimizer):\n",
    "    def train_step(X,Y):\n",
    "        # set model to train mode\n",
    "        model.train()\n",
    "        # forward pass\n",
    "        output_logits, output_softmax = model(X)\n",
    "        predictions = torch.argmax(output_softmax,dim=1)\n",
    "        accuracy = torch.sum(Y==predictions)/float(len(Y))\n",
    "        # compute loss\n",
    "        loss = loss_fnc(output_logits, Y)\n",
    "        # compute gradients\n",
    "        loss.backward()\n",
    "        # update parameters and zero gradients\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        return loss.item(), accuracy*100\n",
    "    return train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-tHuRMZHXSZ6"
   },
   "outputs": [],
   "source": [
    "def make_validate_fnc(model,loss_fnc):\n",
    "    def validate(X,Y):\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            output_logits, output_softmax = model(X)\n",
    "            predictions = torch.argmax(output_softmax,dim=1)\n",
    "            accuracy = torch.sum(Y==predictions)/float(len(Y))\n",
    "            loss = loss_fnc(output_logits,Y)\n",
    "        return loss.item(), accuracy*100, predictions\n",
    "    return validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gtv6tuuVXSZ6"
   },
   "source": [
    "scale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tm3hz4yBXSZ6"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train = np.expand_dims(X_train,1)\n",
    "X_val = np.expand_dims(X_val,1)\n",
    "X_test = np.expand_dims(X_test,1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "b,c,h,w = X_train.shape\n",
    "X_train = np.reshape(X_train, newshape=(b,-1))\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_train = np.reshape(X_train, newshape=(b,c,h,w))\n",
    "\n",
    "b,c,h,w = X_test.shape\n",
    "X_test = np.reshape(X_test, newshape=(b,-1))\n",
    "X_test = scaler.transform(X_test)\n",
    "X_test = np.reshape(X_test, newshape=(b,c,h,w))\n",
    "\n",
    "b,c,h,w = X_val.shape\n",
    "X_val = np.reshape(X_val, newshape=(b,-1))\n",
    "X_val = scaler.transform(X_val)\n",
    "X_val = np.reshape(X_val, newshape=(b,c,h,w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o-Loswb8XSZ6"
   },
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wnmdgZiWXSZ7"
   },
   "outputs": [],
   "source": [
    "EPOCHS=1500\n",
    "DATASET_SIZE = X_train.shape[0]\n",
    "BATCH_SIZE = 32\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Selected device is {}'.format(device))\n",
    "model = ParallelModel(num_emotions=len(EMOTIONS)).to(device)\n",
    "print('Number of trainable params: ',sum(p.numel() for p in model.parameters()) )\n",
    "OPTIMIZER = torch.optim.SGD(model.parameters(),lr=0.01, weight_decay=1e-3, momentum=0.8)\n",
    "\n",
    "train_step = make_train_step(model, loss_fnc, optimizer=OPTIMIZER)\n",
    "validate = make_validate_fnc(model,loss_fnc)\n",
    "losses=[]\n",
    "val_losses = []\n",
    "for epoch in range(EPOCHS):\n",
    "    # schuffle data\n",
    "    ind = np.random.permutation(DATASET_SIZE)\n",
    "    X_train = X_train[ind,:,:,:]\n",
    "    Y_train = Y_train[ind]\n",
    "    epoch_acc = 0\n",
    "    epoch_loss = 0\n",
    "    iters = int(DATASET_SIZE / BATCH_SIZE)\n",
    "    for i in range(iters):\n",
    "        batch_start = i * BATCH_SIZE\n",
    "        batch_end = min(batch_start + BATCH_SIZE, DATASET_SIZE)\n",
    "        actual_batch_size = batch_end-batch_start\n",
    "        X = X_train[batch_start:batch_end,:,:,:]\n",
    "        Y = Y_train[batch_start:batch_end]\n",
    "        X_tensor = torch.tensor(X,device=device).float()\n",
    "        Y_tensor = torch.tensor(Y, dtype=torch.long,device=device)\n",
    "        loss, acc = train_step(X_tensor,Y_tensor)\n",
    "        epoch_acc += acc*actual_batch_size/DATASET_SIZE\n",
    "        epoch_loss += loss*actual_batch_size/DATASET_SIZE\n",
    "        print(f\"\\r Epoch {epoch}: iteration {i}/{iters}\",end='')\n",
    "    X_val_tensor = torch.tensor(X_val,device=device).float()\n",
    "    Y_val_tensor = torch.tensor(Y_val,dtype=torch.long,device=device)\n",
    "    val_loss, val_acc, predictions = validate(X_val_tensor,Y_val_tensor)\n",
    "    losses.append(epoch_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    print('')\n",
    "    print(f\"Epoch {epoch} --> loss:{epoch_loss:.4f}, acc:{epoch_acc:.2f}%, val_loss:{val_loss:.4f}, val_acc:{val_acc:.2f}%\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LR_nciz5XSZ8"
   },
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XYh94T3DXSZ9"
   },
   "outputs": [],
   "source": [
    "SAVE_PATH = os.path.join(os.getcwd(),'models')\n",
    "os.makedirs('models',exist_ok=True)\n",
    "torch.save(model.state_dict(),os.path.join(SAVE_PATH,'cnn_transf_parallel_model.pt'))\n",
    "print('Model is saved to {}'.format(os.path.join(SAVE_PATH,'cnn_transf_parallel_model.pt')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2dLJPgBrXSZ-"
   },
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1SxuUb9BXSZ-"
   },
   "outputs": [],
   "source": [
    "LOAD_PATH = os.path.join(os.getcwd(),'models')\n",
    "model = ParallelModel(len(EMOTIONS))\n",
    "model.load_state_dict(torch.load(os.path.join(LOAD_PATH,'cnn_transf_parallel_model.pt')))\n",
    "print('Model is loaded from {}'.format(os.path.join(LOAD_PATH,'cnn_transf_parallel_model.pt')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5cijTORXSZ_"
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XsVKWvlNXSZ_"
   },
   "outputs": [],
   "source": [
    "X_test_tensor = torch.tensor(X_test,device=device).float()\n",
    "Y_test_tensor = torch.tensor(Y_test,dtype=torch.long,device=device)\n",
    "test_loss, test_acc, predictions = validate(X_test_tensor,Y_test_tensor)\n",
    "print(f'Test loss is {test_loss:.3f}')\n",
    "print(f'Test accuracy is {test_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QTZ7YtspXSZ_"
   },
   "source": [
    "confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iS76pSPtXSZ_"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "\n",
    "predictions = predictions.cpu().numpy()\n",
    "cm = confusion_matrix(Y_test, predictions)\n",
    "names = [EMOTIONS[ind] for ind in range(len(EMOTIONS))]\n",
    "df_cm = pd.DataFrame(cm, index=names, columns=names)\n",
    "# plt.figure(figsize=(10,7))\n",
    "sn.set(font_scale=1.4) # for label size\n",
    "sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}) # font size\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cpYL6c7KXSaA"
   },
   "source": [
    "correlation between emotion intensity and corectness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mce15zq_XSaA"
   },
   "outputs": [],
   "source": [
    "correct_strong = 0\n",
    "correct_normal = 0\n",
    "wrong_strong = 0\n",
    "wrong_normal = 0\n",
    "for i in range(len(X_test)):\n",
    "    intensity = data.loc[test_ind[i],'Emotion intensity']\n",
    "    if Y_test[i] == predictions[i]: # correct prediction\n",
    "        if  intensity == 'normal':\n",
    "            correct_normal += 1\n",
    "        else:\n",
    "            correct_strong += 1\n",
    "    else: # wrong prediction\n",
    "        if intensity == 'normal':\n",
    "            wrong_normal += 1\n",
    "        else:\n",
    "            wrong_strong += 1\n",
    "array = np.array([[wrong_normal,wrong_strong],[correct_normal,correct_strong]])\n",
    "df = pd.DataFrame(array,['wrong','correct'],['normal','strong'])\n",
    "sn.set(font_scale=1.4) # for label size\n",
    "sn.heatmap(df, annot=True, annot_kws={\"size\": 16}) # font size\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ydOrOyMEXSaA"
   },
   "source": [
    "correlation between gender and corectness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oIX8UgVEXSaA"
   },
   "outputs": [],
   "source": [
    "correct_male = 0\n",
    "correct_female = 0\n",
    "wrong_male = 0\n",
    "wrong_female = 0\n",
    "for i in range(len(X_test)):\n",
    "    gender = data.loc[test_ind[i],'Gender']\n",
    "    if Y_test[i] == predictions[i]: # correct prediction\n",
    "        if  gender == 'male':\n",
    "            correct_male += 1\n",
    "        else:\n",
    "            correct_female += 1\n",
    "    else: # wrong prediction\n",
    "        if gender == 'male':\n",
    "            wrong_male += 1\n",
    "        else:\n",
    "            wrong_female += 1\n",
    "array = np.array([[wrong_male,wrong_female],[correct_male,correct_female]])\n",
    "df = pd.DataFrame(array,['wrong','correct'],['male','female'])\n",
    "sn.set(font_scale=1.4) # for label size\n",
    "sn.heatmap(df, annot=True, annot_kws={\"size\": 16}) # font size\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GGx_5F6GXSaB"
   },
   "source": [
    "# Plot loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jirP-QhRXSaB"
   },
   "outputs": [],
   "source": [
    "plt.plot(losses,'b')\n",
    "plt.plot(val_losses,'r')\n",
    "plt.legend(['train loss','val loss'])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "parallel-cnn-transformer.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
